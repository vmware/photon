From a8d952896cf3dde0ea252a173031e3a6b1f0762e Mon Sep 17 00:00:00 2001
From: Alexey Makhalov <amakhalov@vmware.com>
Date: Mon, 7 Aug 2023 16:48:33 +0000
Subject: [PATCH] Enable and enhance SCHED isolation

Upstream https://github.com/vmware/photon/commit/399e9d5a879459f5964509b255ce83f304b5326e
commit

CPU isolation code has several types of users or kernel subsystem such
as a sched domain, timer, workqueue, etc. See enum hk_flags in
include/linux/sched/isolation.h for full list of isolation types.

Separation on several types was introduced long time ago by upstream
commit de201559df872f83d0c08 ("sched/isolation: Introduce housekeeping
flags"). This commit introduced HK_FLAG_SCHED type, but there was no
path of kernel paramater to activate it. HK_FLAG_SCHED isolation remains
dead code even now. Some aspects of scheduler isolation to avoid processes
on isolcpus such as migration or load balancing disablement on isolcpus
are covered by HK_FLAG_DOMAIN. But there are still cases where we can see
processes scheduled on isolcpus. Example, initial placement of the
process when its cpuset is mix of isol and non isol cpus can pick isolcpu.
The process will remain there for entire life of until user force move it.
It won't be automatically migrated as load balancer does not touch
isolcpus.

This patch enables HK_FLAG_SCHED and enhances corresponding sched isolation
to do not put any processes with fair sched class to isolcpus. The main
goal of this patch is to improve initial placement of the processes within
a cpuset. There are 3 possible scenarios:
 1. No isolcpus within a target cpuset - pick first or any cpu, depending
    on enablement of distribution logic.
 2. Some CPUs within cpuset are isolated - pick a non isolated CPU.
 3. All CPUs within cpuset are isolated - pick first or any cpum depending
    on enablement of distribution logic.
Note, sched isolation only works for non realtime processes with fair
scheduling class.

To activate sched isolation "sched" flag must be passed to the isolcpus=
kernel parameter. Example: "isolcpus=domain,sched,managed_irq,2-3"

Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
[Ankit: Forward-port for v6.6]
Signed-off-by: Ankit Jain <ankit-aj.jain@broadcom.com>
---
 kernel/sched/core.c      | 11 +++++++++++
 kernel/sched/fair.c      | 23 ++++++++++++++++++++++-
 kernel/sched/isolation.c |  6 ++++++
 3 files changed, 39 insertions(+), 1 deletion(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index c64a2f667..f2c9c9458 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3136,6 +3136,8 @@ static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 {
 	const struct cpumask *cpu_allowed_mask = task_cpu_possible_mask(p);
 	const struct cpumask *cpu_valid_mask = cpu_active_mask;
+	struct cpumask cpu_valid_submask;
+	const struct cpumask *hk_mask = housekeeping_cpumask(HK_TYPE_SCHED);
 	bool kthread = p->flags & PF_KTHREAD;
 	unsigned int dest_cpu;
 	int ret = 0;
@@ -3185,6 +3187,15 @@ static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 		}
 	}
 
+	/*
+	 * dest_cpu should not use isolcpus if housekeeping cpus are present in
+	 * new_mask. Reduce cpu_valid_mask to housekeeping subset.
+	 */
+	if (cpumask_intersects(ctx->new_mask, hk_mask)) {
+		cpumask_and(&cpu_valid_submask, cpu_valid_mask, hk_mask);
+		cpu_valid_mask = &cpu_valid_submask;
+	}
+
 	/*
 	 * Picking a ~random cpu helps in cases where we are changing affinity
 	 * for groups of tasks (ie. cpuset), so that load balancing is not
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index ed7894d27..82b1d001c 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6625,6 +6625,9 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 	int idle_h_nr_running = task_has_idle_policy(p);
 	int task_new = !(flags & ENQUEUE_WAKEUP);
 
+	WARN(!housekeeping_test_cpu(cpu_of(rq), HK_TYPE_SCHED) &&
+		!(p->flags & PF_KTHREAD),
+		"Enqueue %s(%d) on cpu %d", p->comm, p->pid, cpu_of(rq));
 	/*
 	 * The code below (indirectly) updates schedutil which looks at
 	 * the cfs_rq utilization to select a frequency.
@@ -8144,6 +8147,24 @@ static void task_dead_fair(struct task_struct *p)
 	remove_entity_load_avg(&p->se);
 }
 
+static void
+set_cpus_allowed_fair(struct task_struct *p, struct affinity_context *ctx)
+{
+	if (housekeeping_enabled(HK_TYPE_SCHED) &&
+	    !(p->flags & PF_KTHREAD) &&
+	    !(ctx->flags & (SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE))) {
+		const struct cpumask *hk_mask = housekeeping_cpumask(HK_TYPE_SCHED);
+
+		if (cpumask_intersects(ctx->new_mask, hk_mask)) {
+			cpumask_and(&p->cpus_mask, ctx->new_mask, hk_mask);
+			p->nr_cpus_allowed = cpumask_weight(&p->cpus_mask);
+			return;
+		}
+	}
+
+	set_cpus_allowed_common(p, ctx);
+}
+
 static int
 balance_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 {
@@ -13046,7 +13067,7 @@ DEFINE_SCHED_CLASS(fair) = {
 	.rq_offline		= rq_offline_fair,
 
 	.task_dead		= task_dead_fair,
-	.set_cpus_allowed	= set_cpus_allowed_common,
+	.set_cpus_allowed	= set_cpus_allowed_fair,
 #endif
 
 	.task_tick		= task_tick_fair,
diff --git a/kernel/sched/isolation.c b/kernel/sched/isolation.c
index 373d42c70..e4da154c6 100644
--- a/kernel/sched/isolation.c
+++ b/kernel/sched/isolation.c
@@ -202,6 +202,12 @@ static int __init housekeeping_isolcpus_setup(char *str)
 			continue;
 		}
 
+		if (!strncmp(str, "sched,", 6)) {
+			str += 6;
+			flags |= HK_FLAG_SCHED;
+			continue;
+		}
+
 		if (!strncmp(str, "domain,", 7)) {
 			str += 7;
 			flags |= HK_FLAG_DOMAIN;
-- 
2.39.4

