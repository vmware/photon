diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index 68a2d76e..82c02ab 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -53,6 +53,16 @@ ENTRY(native_usergs_sysret64)
 END(native_usergs_sysret64)
 #endif /* CONFIG_PARAVIRT */
 
+.macro pax_rand_kstack
+#ifdef CONFIG_PAX_RANDKSTACK
+	pushq	%rax
+	pushq	%r11
+	call	pax_randomize_kstack
+	popq	%r11
+	popq	%rax
+#endif
+.endm
+
 .macro TRACE_IRQS_IRETQ
 #ifdef CONFIG_TRACE_IRQFLAGS
 	bt	$9, EFLAGS(%rsp)		/* interrupts off? */
@@ -232,6 +242,8 @@ GLOBAL(entry_SYSCALL_64_after_hwframe)
 	movq	%rsp, %rdi
 	call	do_syscall_64		/* returns with IRQs disabled */
 
+	pax_rand_kstack
+
 	TRACE_IRQS_IRETQ		/* we're about to change IF */
 
 	/*
@@ -394,6 +406,7 @@ ENTRY(ret_from_fork)
 	UNWIND_HINT_REGS
 	movq	%rsp, %rdi
 	call	syscall_return_slowpath	/* returns with IRQs disabled */
+	pax_rand_kstack
 	TRACE_IRQS_ON			/* user mode is traced as IRQS on */
 	jmp	swapgs_restore_regs_and_return_to_usermode
 
@@ -584,6 +597,7 @@ ret_from_intr:
 GLOBAL(retint_user)
 	mov	%rsp,%rdi
 	call	prepare_exit_to_usermode
+	pax_rand_kstack
 	TRACE_IRQS_IRETQ
 
 GLOBAL(swapgs_restore_regs_and_return_to_usermode)
diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 15fc074..3cfe1e6 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -819,12 +819,21 @@ static inline void spin_lock_prefetch(const void *x)
 
 #define task_top_of_stack(task) ((unsigned long)(task_pt_regs(task) + 1))
 
+#ifdef CONFIG_PAX_RANDKSTACK
+#define task_pt_regs(task) \
+({                                                                      \
+        unsigned long __ptr = (unsigned long)task_stack_page(task);     \
+        __ptr += THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING - 16;        \
+        ((struct pt_regs *)__ptr) - 1;                                  \
+})
+#else
 #define task_pt_regs(task) \
 ({									\
 	unsigned long __ptr = (unsigned long)task_stack_page(task);	\
 	__ptr += THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;		\
 	((struct pt_regs *)__ptr) - 1;					\
 })
+#endif
 
 #ifdef CONFIG_X86_32
 /*
diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 9eb448c..7979a1d 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -698,3 +698,17 @@ unsigned long KSTK_ESP(struct task_struct *task)
 {
 	return task_pt_regs(task)->sp;
 }
+
+#ifdef CONFIG_PAX_RANDKSTACK
+void pax_randomize_kstack(void)
+{
+	struct task_struct *task = current;
+	unsigned long time;
+
+	if (!randomize_va_space)
+		return;
+
+	time = rdtsc() & 0xFUL;
+	load_sp0(task_top_of_stack(task)^(time << 4));
+}
+#endif
diff --git a/security/Kconfig b/security/Kconfig
index 0dec261..08d4882 100644
--- a/security/Kconfig
+++ b/security/Kconfig
@@ -80,6 +80,22 @@ config PAX_MPROTECT
 	  the enforcement of non-executable pages.
 
 endif
+
+config PAX_RANDKSTACK
+	bool "Randomize kernel stack base"
+	depends on X86_TSC && X86
+	help
+	  By saying Y here the kernel will randomize every task's kernel
+	  stack on every system call.  This will not only force an attacker
+	  to guess it but also prevent him from making use of possible
+	  leaked information about it.
+
+	  Since the kernel stack is a rather scarce resource, randomization
+	  may cause unexpected stack overflows, therefore you should very
+	  carefully test your system.  Note that once enabled in the kernel
+	  configuration, this feature cannot be disabled on a per file basis.
+
+
 endif
 
 source security/keys/Kconfig
