From 6dca4744eec08e19aaf344399833d9f482924267 Mon Sep 17 00:00:00 2001
From: Keerthana K <keerthanak@vmware.com>
Date: Wed, 29 Mar 2023 10:40:59 +0000
Subject: [PATCH 3/8] aesni_intel: Remove static call

Signed-off-by: Keerthana K <keerthanak@vmware.com>
---
 arch/x86/crypto/aesni-intel_glue.c | 47 ++++++++++++++++++------------
 1 file changed, 28 insertions(+), 19 deletions(-)

diff --git a/arch/x86/crypto/aesni-intel_glue.c b/arch/x86/crypto/aesni-intel_glue.c
index 10420b2aa..0628c2bca 100644
--- a/arch/x86/crypto/aesni-intel_glue.c
+++ b/arch/x86/crypto/aesni-intel_glue.c
@@ -34,7 +34,6 @@
 #include <linux/jump_label.h>
 #include <linux/workqueue.h>
 #include <linux/spinlock.h>
-#include <linux/static_call.h>
 #include <crypto/gf128mul.h>
 
 void fcw_kernel_fpu_begin(void);
@@ -111,9 +110,11 @@ asmlinkage void aesni_xts_decrypt(const struct crypto_aes_ctx *ctx, u8 *out,
 
 #ifdef CONFIG_X86_64
 
+static void (*aesni_ctr_enc_tfm)(struct crypto_aes_ctx *ctx, u8 *out,
+				const u8 *in, unsigned int len, u8 *iv);
+
 asmlinkage void aesni_ctr_enc(struct crypto_aes_ctx *ctx, u8 *out,
 			      const u8 *in, unsigned int len, u8 *iv);
-DEFINE_STATIC_CALL(aesni_ctr_enc_tfm, aesni_ctr_enc);
 
 /* Scatter / Gather routines, with args similar to above */
 asmlinkage void aesni_gcm_init(void *ctx,
@@ -131,7 +132,6 @@ asmlinkage void aesni_gcm_dec_update(void *ctx,
 asmlinkage void aesni_gcm_finalize(void *ctx,
 				   struct gcm_context_data *gdata,
 				   u8 *auth_tag, unsigned long auth_tag_len);
-
 asmlinkage void aes_ctr_enc_128_avx_by8(const u8 *in, u8 *iv,
 		void *keys, u8 *out, unsigned int num_bytes);
 asmlinkage void aes_ctr_enc_192_avx_by8(const u8 *in, u8 *iv,
@@ -175,6 +175,15 @@ asmlinkage void aesni_gcm_finalize_avx_gen2(void *ctx,
 				   struct gcm_context_data *gdata,
 				   u8 *auth_tag, unsigned long auth_tag_len);
 
+/*
+static const struct aesni_gcm_tfm_s aesni_gcm_tfm_avx_gen2 = {
+	.init = &aesni_gcm_init_avx_gen2,
+	.enc_update = &aesni_gcm_enc_update_avx_gen2,
+	.dec_update = &aesni_gcm_dec_update_avx_gen2,
+	.finalize = &aesni_gcm_finalize_avx_gen2,
+};
+*/
+
 /*
  * asmlinkage void aesni_gcm_init_avx_gen4()
  * gcm_data *my_ctx_data, context data
@@ -198,8 +207,8 @@ asmlinkage void aesni_gcm_finalize_avx_gen4(void *ctx,
 				   struct gcm_context_data *gdata,
 				   u8 *auth_tag, unsigned long auth_tag_len);
 
-static __ro_after_init DEFINE_STATIC_KEY_FALSE(gcm_use_avx);
-static __ro_after_init DEFINE_STATIC_KEY_FALSE(gcm_use_avx2);
+static __ro_after_init int gcm_use_avx = 0;
+static __ro_after_init int gcm_use_avx2 = 0;
 
 static inline struct
 aesni_rfc4106_gcm_ctx *aesni_rfc4106_gcm_ctx_get(struct crypto_aead *tfm)
@@ -524,10 +533,9 @@ static int ctr_crypt(struct skcipher_request *req)
 	while ((nbytes = walk.nbytes) > 0) {
 		fcw_kernel_fpu_begin();
 		if (nbytes & AES_BLOCK_MASK)
-			static_call(aesni_ctr_enc_tfm)(ctx, walk.dst.virt.addr,
-						       walk.src.virt.addr,
-						       nbytes & AES_BLOCK_MASK,
-						       walk.iv);
+			aesni_ctr_enc_tfm(ctx, walk.dst.virt.addr, walk.src.virt.addr,
+						nbytes & AES_BLOCK_MASK, walk.iv);
+
 		nbytes &= ~AES_BLOCK_MASK;
 
 		if (walk.nbytes == walk.total && nbytes > 0) {
@@ -712,10 +720,10 @@ static int gcmaes_crypt_by_sg(bool enc, struct aead_request *req,
 	}
 
 	fcw_kernel_fpu_begin();
-	if (static_branch_likely(&gcm_use_avx2) && do_avx2)
+	if (likely(gcm_use_avx2) && do_avx2)
 		aesni_gcm_init_avx_gen4(aes_ctx, data, iv, hash_subkey, assoc,
 					assoclen);
-	else if (static_branch_likely(&gcm_use_avx) && do_avx)
+	else if (likely(gcm_use_avx) && do_avx)
 		aesni_gcm_init_avx_gen2(aes_ctx, data, iv, hash_subkey, assoc,
 					assoclen);
 	else
@@ -732,7 +740,7 @@ static int gcmaes_crypt_by_sg(bool enc, struct aead_request *req,
 
 	while (walk.nbytes > 0) {
 		fcw_kernel_fpu_begin();
-		if (static_branch_likely(&gcm_use_avx2) && do_avx2) {
+		if (likely(gcm_use_avx2) && do_avx2) {
 			if (enc)
 				aesni_gcm_enc_update_avx_gen4(aes_ctx, data,
 							      walk.dst.virt.addr,
@@ -743,7 +751,7 @@ static int gcmaes_crypt_by_sg(bool enc, struct aead_request *req,
 							      walk.dst.virt.addr,
 							      walk.src.virt.addr,
 							      walk.nbytes);
-		} else if (static_branch_likely(&gcm_use_avx) && do_avx) {
+		} else if (likely(gcm_use_avx) && do_avx) {
 			if (enc)
 				aesni_gcm_enc_update_avx_gen2(aes_ctx, data,
 							      walk.dst.virt.addr,
@@ -770,10 +778,10 @@ static int gcmaes_crypt_by_sg(bool enc, struct aead_request *req,
 		return err;
 
 	fcw_kernel_fpu_begin();
-	if (static_branch_likely(&gcm_use_avx2) && do_avx2)
+	if (likely(gcm_use_avx2) && do_avx2)
 		aesni_gcm_finalize_avx_gen4(aes_ctx, data, auth_tag,
 					    auth_tag_len);
-	else if (static_branch_likely(&gcm_use_avx) && do_avx)
+	else if (likely(gcm_use_avx) && do_avx)
 		aesni_gcm_finalize_avx_gen2(aes_ctx, data, auth_tag,
 					    auth_tag_len);
 	else
@@ -1248,18 +1256,19 @@ static int __init aesni_init(void)
 #ifdef CONFIG_X86_64
 	if (boot_cpu_has(X86_FEATURE_AVX2)) {
 		pr_info("AVX2 version of gcm_enc/dec engaged.\n");
-		static_branch_enable(&gcm_use_avx);
-		static_branch_enable(&gcm_use_avx2);
+		gcm_use_avx = 1;
+		gcm_use_avx2 = 1;
 	} else
 	if (boot_cpu_has(X86_FEATURE_AVX)) {
 		pr_info("AVX version of gcm_enc/dec engaged.\n");
-		static_branch_enable(&gcm_use_avx);
+		gcm_use_avx = 1;
 	} else {
 		pr_info("SSE version of gcm_enc/dec engaged.\n");
 	}
+	aesni_ctr_enc_tfm = aesni_ctr_enc;
 	if (boot_cpu_has(X86_FEATURE_AVX)) {
 		/* optimize performance of ctr mode encryption transform */
-		static_call_update(aesni_ctr_enc_tfm, aesni_ctr_enc_avx_tfm);
+		aesni_ctr_enc_tfm = aesni_ctr_enc_avx_tfm;
 		pr_info("AES CTR mode by8 optimization enabled\n");
 	}
 #endif /* CONFIG_X86_64 */
-- 
2.41.0
